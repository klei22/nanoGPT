%%%%%%%% ICML 2025 SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

% For math and theorem environments
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Top-1 Attenuated Losses for Autoregressive Language Models}

\begin{document}

\twocolumn[
\icmltitle{Top-1 Attenuated Losses Improve Autoregressive Language Modeling}

\begin{icmlauthorlist}
\icmlauthor{Jane Doe}{yyy}
\icmlauthor{John Smith}{comp}
\icmlauthor{Alex Kim}{comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science, Example University, Example City, Country}
\icmlaffiliation{comp}{Example Research, Example City, Country}

\icmlcorrespondingauthor{Jane Doe}{jane.doe@example.edu}

\icmlkeywords{Language Modeling, Loss Functions, Target Rank}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
Top-1 focused loss functions provide a principled way to align autoregressive language models with the ranking-sensitive metrics used at inference time.
We revisit recently introduced variants---skip correct top-1, attenuated correct top-1, and distance attenuated top-1---and compare them against cross-entropy and focal loss.
Our study highlights how selectively reducing gradients on already correct predictions can improve target rank and left-to-right probability calibration while maintaining training stability.
We summarize preliminary empirical findings showing that skip correct top-1 and distance attenuated top-1 with strength $0.75$ outperform cross-entropy on the targeted metrics, motivating further large-scale evaluation.
\end{abstract}

\section{Introduction}
Autoregressive language models are typically optimized with token-level cross-entropy, yet downstream evaluation emphasizes top-1 accuracy and the probability mass assigned to the correct continuation.
This mismatch can leave improvements in target rank untapped.
Recent work has proposed top-$k$ calibrated objectives \citep{lapin2016topk} and focal weighting schemes \citep{lin2017focal} to rebalance gradients towards challenging examples.
We build upon this line of research by detailing several loss variants that attenuate or omit gradients for tokens already predicted correctly at top-1.
Our goal is to document the formulation of these losses, provide intuition for their behavior, and report preliminary findings using the exploration sweep introduced in the repository.

\section{Background and Related Work}
Cross-entropy remains the de facto training objective for autoregressive models \citep{goodfellow2016deep}, encouraging calibrated likelihoods over the full vocabulary.
However, applications such as next-token prediction and decoding rely heavily on the identity and confidence of the highest-probability token.
Prior studies on top-$k$ optimization \citep{lapin2016topk} and focal loss \citep{lin2017focal} demonstrate that emphasizing hard examples can reduce ranking errors.
Our work adapts these insights to the transformer-based nanoGPT setting by exploring objectives that explicitly modulate the contribution of already correct top-1 predictions.

\section{Methods}
We consider a batch of logits $\mathbf{z}_{t} \in \mathbb{R}^{V}$ for timestep $t$ and vocabulary size $V$.
Softmax probabilities are $p_{t,i} = \exp(z_{t,i}) / \sum_{j} \exp(z_{t,j})$, and the ground-truth token index is $y_t$.
Let $\mathcal{I}$ denote the set of valid timesteps (i.e., tokens with target label not equal to the ignore index $-1$).
For each timestep we define the top-1 prediction $\hat{y}_t = \arg\max_i z_{t,i}$ and the associated correctness indicator $c_t = \mathbf{1}[\hat{y}_t = y_t]$.

\subsection{Cross-Entropy Loss}
The standard objective is
\begin{equation}
\label{eq:ce}
\mathcal{L}_{\mathrm{CE}} = - \frac{1}{|\mathcal{I}|} \sum_{t \in \mathcal{I}} \log p_{t,y_t},
\end{equation}
which penalizes low probability on the correct token uniformly across examples.

\subsection{Focal Loss}
Following \citet{lin2017focal}, focal loss down-weights easy examples via a focusing parameter $\gamma$:
\begin{equation}
\label{eq:focal}
\mathcal{L}_{\mathrm{Focal}} = - \frac{1}{|\mathcal{I}|} \sum_{t \in \mathcal{I}} \left(1 - p_{t,y_t}\right)^{\gamma} \log p_{t,y_t}.
\end{equation}
Larger $\gamma$ emphasizes low-confidence predictions by shrinking the contribution of already confident tokens.

\subsection{Skip Correct Top-1 Loss}
The skip correct top-1 loss entirely removes contributions from timesteps that are already top-1 correct:
\begin{equation}
\label{eq:skip}
\mathcal{L}_{\mathrm{Skip}} = - \frac{1}{\sum_{t \in \mathcal{I}} (1 - c_t)} \sum_{t \in \mathcal{I}} (1 - c_t) \log p_{t,y_t}.
\end{equation}
When all predictions in a batch are correct we define the expression to evaluate to zero, effectively pausing updates.
This variant concentrates optimization on misranked targets and proved stable in our experiments even with aggressive learning-rate schedules.

\subsection{Attenuated Correct Top-1 Loss}
Rather than discarding gradients, the attenuated correct top-1 loss scales them by a constant attenuation factor $\alpha \in [0,1]$:
\begin{equation}
\label{eq:attenuated}
\mathcal{L}_{\mathrm{Att}} = - \frac{1}{|\mathcal{I}|} \sum_{t \in \mathcal{I}} \big[ \alpha + (1-\alpha)(1 - c_t) \big] \log p_{t,y_t}.
\end{equation}
Setting $\alpha=1$ recovers cross-entropy, while smaller $\alpha$ progressively reduces the contribution from tokens already predicted correctly.
This keeps gradients non-zero for all tokens, which can aid optimizer stability relative to the skip variant.

\subsection{Distance Attenuated Top-1 Loss}
The distance attenuated top-1 loss modulates attenuation by the logit gap between the predicted top token and the target token.
Let $\Delta_t = \max_i z_{t,i} - z_{t,y_t} \ge 0$ denote this gap.
We define an attenuation factor using a strength hyperparameter $\lambda \ge 0$:
\begin{equation}
\label{eq:distance}
\beta_t = 1 - \lambda \exp(-\Delta_t), \qquad \mathcal{L}_{\mathrm{Dist}} = - \frac{1}{|\mathcal{I}|} \sum_{t \in \mathcal{I}} \beta_t \log p_{t,y_t}.
\end{equation}
Tokens with small $\Delta_t$ (i.e., near-correct predictions) receive reduced gradients, whereas poorly ranked tokens maintain close-to-cross-entropy weighting.
In the reported experiments we sweep $\lambda \in \{0.0, 0.25, 0.5, 0.75\}$ and find that $\lambda=0.75$ offers the best trade-off between stability and rank-focused improvement.

\section{Experimental Setup}
We reuse the exploration sweep defined in \texttt{explorations/skip\_correct\_top1\_comparison.yaml}, which contrasts the proposed losses under consistent architectural and optimization hyperparameters.
The sweep trains 6-layer, 6-head transformer models with 384-dimensional embeddings over 10k iterations, sharing rotary position encodings, hyperspherical normalization layers, and mixed precision (bfloat16) execution across all runs.
Datasets include both \'opus-100\' and \'minipile\', allowing us to gauge behavior on multilingual and open-domain corpora.
We evaluate every 500 iterations and log target rank (the ordinal position of the correct token under the model distribution) alongside the left-to-right probability metric derived from autoregressive rollouts.
Loss-specific hyperparameters follow the YAML specification: attenuation factors $\alpha \in \{0.25, 0.5, 0.75, 1.0\}$, distance strengths $\lambda \in \{0.0, 0.25, 0.5, 0.75\}$, and focal exponents $\gamma \in \{1.0, 2.0, 4.0\}$.

\section{Results}
Skip correct top-1 and distance attenuated top-1 with $\lambda = 0.75$ consistently improved the median target rank and boosted left probability relative to cross-entropy across both datasets.
The skip variant yielded the largest gains on target rank by sharply concentrating updates on mispredicted tokens, while the distance-based attenuation provided smoother training curves and comparable left-probability improvements.
Focal loss improved over cross-entropy on early iterations but lagged behind the top-1 targeted objectives once the model achieved high accuracy.
\todo[inline]{Insert quantitative table summarizing validation metrics by loss function.}
\todo[inline]{Add figure illustrating evolution of target rank throughout training.}

\section{Discussion}
These findings suggest that attenuating gradients for already-correct predictions can reconcile the objective with ranking-based evaluation without destabilizing optimization.
The skip variant trades off gradient coverage for sharper focus, which could interact with curriculum schedules and sampling noise; the distance attenuated approach mitigates this by smoothly interpolating between cross-entropy and skip behavior.
Future work should investigate larger-scale datasets, longer schedules, and integration with decoding-time reranking mechanisms.
\todo[inline]{Discuss ablation on attenuation hyperparameters beyond the current sweep.}

\section*{Impact Statement}
This work advances training techniques for language models, which can be applied across a range of natural language generation tasks.
Potential risks align with existing concerns around large-scale language models, including misuse for disinformation or the reinforcement of biases present in training data.
Exploring top-1 focused losses does not introduce new ethical considerations beyond those already recognized for autoregressive modeling.

\section*{Acknowledgements}
\todo[inline]{Add acknowledgements once author information is finalized.}

\bibliography{titleofreport}
\bibliographystyle{icml2025}

\appendix
\onecolumn
\section{Additional Experimental Details}
\todo[inline]{Provide extended training curves, hyperparameter grids, and configuration listings.}

\end{document}
