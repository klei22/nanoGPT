\documentclass{article}

% If you want compressed numeric citations:
\PassOptionsToPackage{numbers,compress}{natbib}

% === NeurIPS 2025 Workshop (double-blind) style ===
\usepackage[dblblindworkshop]{titleofreport}
\title{NeurIPS 2025 Workshop on Mechanistic Interpretability}

% === Title (double-blind; authors anonymized) ===
\title{Geometry-Preserving Transforms based on the Hypersphere Interpretation of LLMs}

\author{Anonymous Authors}

% === Common packages (safe with titleofreport) ===
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath, amssymb, mathtools, amsthm}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}

% === Math macros ===
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\ip}[2]{\left\langle #1,\,#2 \right\rangle}
\newcommand{\Trans}{\mathsf{T}}

\newtheorem{lemma}{Lemma}

\begin{document}
\maketitle

\begin{abstract}
A growing body of research suggests that Transformers, the architecture
underpinning large-language-models, can be interpreted as being composed of
weight vectors and latent vectors in space of the embedding dimensionality, and
furthermore that we can think of operations as angular rotations of the
activation vectors along the surface of a hypersphere. This is supported by the
success of steering vectors, linear activation probes, SLERP based model merging
strategies, and the progressively increasing number of normalization steps in
modern model architectures. In this work we run with this interpretation of the
model weights and activations as vectors in a high dimensional space, and
explore transforms which preserve the geometry and relative angles of vectors
within the model. This transform's efficacy draws from the Johnson-Lindenstrauss
lemma, suggesting this technique should work better for larger dimensional
models, and our hypothesis is that this should preserve geometric relationships
of weight vectors, and retain language modeling capability of the model. While
we find that with certain Johnson-Lindenstrauss transforms, that we can indeed
grow or shrink the model size along the embedding dimension, that language
modeling abilities are only recovered after brief finetuning. Thus we further
explored architecture modifications which minimize the amount of post-transform
finetuning needed recover language modeling abilities, as well as optimizations
to the transform algorithm, and provide comparisons of both the relative latent
space of the transformed model with the original model. All code and scripts for
replicating these results for experiments are available at the following url:
\url{https://anonymous.4open.science/r/jltransform/README.md}
\end{abstract}
 
\section{Introduction}
\label{sec:intro}
Mechanistic interpretability has yielded many insights into the inner mechanisms
of LLMs, and of these is a stronger and stronger interpretation of both weights
and activations as semantically meaningful vectors in a high dimensional space.
Recent developments have started forming towards interpreting these vectors as
living on or near the surface of a hypersphere, or in that the magnitude of the
final residual vectors is less important than the angle. We can interpret
vectors of Attn and FFN up-projections as searching for features in the
residual, down-projections as adding new vectors of information to the residual
vector, and normalizations as re-mapping to the hypersphere. Inputs of Attn and
FFN modules due to pre-norm essentially only see hypersphere normalized data, we
also normalized before the final LM Head projection -- resulting in favoring the
sub-word or token with highest cosine similarity with the residual.

We see further normalization in modern networks with Peri-LN of Gemma models and
Olmo, adding additional normalization to the output of attention and ffn
modules, as well as NormalizedGPT, which in addition to directly enforcing per
iteration, utilizes a SLERP instead of addition for skip connections, something
mathematically equivalent to prior post-norm architectures (which though more
performant than pre-norm architectures, were less stable).

Thus we conjecture if the true information of the model is contained in the
geometry and especially the relative angles of both weight vectors and latent
activation vectors of the model, then \textit{transformations that preserve
these geometric relationships should also conserve the model's language
capabilities}.

In this line of thought, we show that we can more fluidly change the embedding
dimension of trained Transformers. We treat token embeddings, and all tensors as
composed of vectors on within $R^d$, where $d$ is the embedding dimension of the
model, and then perform JL projection $T:\R^d\!\to\!\R^m$, we demonstrate that
this approximately preserves pairwise angles and distances with high probability
for a finite set, and and recovers performance of the original model with minimal
finetuning steps.

\paragraph{Contributions.}
\begin{itemize}
  \item \textbf{Embedding Dimension Fluidity} We show that we can successfully
  shrink or grow the dimensionality in pre-trained language models, and then
  recover prior language modeling performance after minimal finetuning steps.
  \item \textbf{Model Architecture Augmentations} We share results and
  interpretations of model architecture changes which greatly reduce the already
  finetuning steps required to recover performance after transforming.
  \item \textbf{JL Transform Variations} We share comparisons and algorithms
  \item \textbf{Mechanistic metrics \& protocol.} We propose geometry-centric
  metrics (angular error, hyperspherical concordance) and a minimal protocol to
  evaluate mechanistic fidelity pre/post LGPT alongside perplexity recovery.
  \item \textbf{Replication} We provide a NanoGPT based repo with scripts and
  modular framework for replication and extension of this technique and
  exploring model architecture compatibilities.
\end{itemize}

\section{Background: a Hypersphere Lens}
\label{sec:background}
\subsection{Hypersphere view of transformer states}
LayerNorm/RMSNorm and cosine-based attention suggest analyzing features and states
on (or near) a unit sphere. Let $x\in\R^d$ denote a hidden state and
$\hat{x}=x/\norm{x}$ its normalized representative on $\mathbb{S}^{d-1}$. Under
this lens, cosine similarity $\ip{\hat{x}}{\hat{y}}$ and angular distance
$\arccos(\ip{\hat{x}}{\hat{y}})$ are the primary geometric objects, while norms
are factored out by normalization. Many mechanistic probes (e.g., steering vectors,
projections onto feature directions) are intrinsically angular.

\subsection{Johnson--Lindenstrauss lemma}
\label{ssec:jl}
\begin{lemma}[Johnson--Lindenstrauss]
\label{lem:jl}
Let $X=\{x_1,\dots,x_N\}\subset\R^d$ and $0<\varepsilon<1$. There exists a linear
map $T:\R^d\to\R^m$ with $m=\mathcal{O}(\varepsilon^{-2}\log N)$ such that for all
$i,j$,
\[
(1-\varepsilon)\,\norm{x_i-x_j}^2 \le \norm{T x_i - T x_j}^2
\le (1+\varepsilon)\,\norm{x_i-x_j}^2.
\]
Moreover, a random matrix $T$ drawn from simple distributions (e.g., Gaussian
entries $\mathcal{N}(0,1/m)$) satisfies this with high probability.
\end{lemma}
Because $\norm{u-v}^2=\norm{u}^2+\norm{v}^2-2\ip{u}{v}$, preserving distances
implies approximate preservation of inner products and hence angles (especially
after per-vector normalization). For mechanistic interpretability, $X$ may consist
of token embeddings, steering vectors, and a representative sample of hidden
states from a calibration corpus.

\section{Method: Latent Geometry-Preserving Transform (LGPT)}
\label{sec:method}
Let $d$ be the original width and $m<d$ the target width. We choose a \emph{single}
projection $T\in\R^{m\times d}$ and a left pseudo-inverse $T^+\in\R^{d\times m}$
(defined below), and map any hidden state $x$ to $z=Tx$.

\subsection{Commuting construction and porting rules}
We seek layer replacements $\tilde{f}$ so that for $z=Tx$,
\begin{equation}
\label{eq:commute}
\tilde{f}(z) \approx T f(x),
\end{equation}
ensuring computations in the reduced model correspond to the projected computations
of the original. For affine $y=Ax+b$ with $A\in\R^{d_\text{out}\times d}$,
the reduced operator uses the \emph{sandwich rule}
\begin{equation}
\label{eq:sandwich}
\tilde{A} = T A T^+,\qquad \tilde{b}=Tb.
\end{equation}
When $T$ has full row rank, a canonical left pseudo-inverse is
\begin{equation}
\label{eq:pseudoinv}
T^+ = T^\Trans (TT^\Trans)^{-1}.
\end{equation}
If $TT^\Trans=I_m$ (row-orthonormal $T$), then $T^+=T^\Trans$.

\paragraph{Embeddings (rows are token vectors).}
For $E\in\R^{V\times d}$, set
\[
\tilde{E} = E\,T^\Trans\in\R^{V\times m}.
\]
\paragraph{Self-attention.}
Using the common $(d\!\to\!d)$ convention,
\[
\tilde{W}_Q=T W_Q T^+,\quad
\tilde{W}_K=T W_K T^+,\quad
\tilde{W}_V=T W_V T^+,\quad
\tilde{W}_O=T W_O T^+.
\]
Require $m$ divisible by $n_\text{heads}$ so $\tilde{d}_\text{head}=m/n_\text{heads}$;
update the scale to $1/\sqrt{\tilde{d}_\text{head}}$.

\paragraph{Feed-forward (MLP).}
For $W_\text{in}\in\R^{d\times d_\text{ff}}$ and $W_\text{out}\in\R^{d_\text{ff}\times d}$,
\[
\tilde{W}_\text{in}=T W_\text{in},\qquad
\tilde{W}_\text{out}=W_\text{out} T^+.
\]
Gated variants (e.g., SwiGLU) apply per branch.

\paragraph{Norms.}
Per-dimension gains $\gamma\in\R^d$ are basis-dependent. Either (i) drop per-dim gains
(use gain $=1$) while keeping centering/variance, or (ii) replace with a \emph{single}
scalar gain per layer. This avoids dense $T\,\mathrm{Diag}(\gamma)\,T^+$.

\paragraph{Positional encoding.}
ALiBi is width-agnostic. For RoPE, set the rotary dimension to $\tilde{d}_\text{head}$
(and keep base unless re-tuning).

\paragraph{LM head.}
If untied, $W_\text{vocab}\in\R^{V\times d}$ maps to $\tilde{W}_\text{vocab}=W_\text{vocab}T^+$.
If tied, prefer constructions where $T^+=T^\Trans$.

\subsection{Gaussian JL transform (construction and pseudocode)}
The simplest JL variant draws $T_{ij}\sim\mathcal{N}(0,1/m)$ independently.
For any fixed $u$, $\norm{Tu}^2$ concentrates around $\norm{u}^2$ with sub-Gaussian
tails. A union bound over the calibration set yields Lemma~\ref{lem:jl}.

\begin{algorithm}[H]
\caption{Gaussian JL Construction and Model Porting (LGPT)}
\label{alg:gaussian-jl}
\begin{algorithmic}[1]
\REQUIRE Original width $d$, target width $m<d$, calibration size $N$, distortion $\varepsilon$, failure prob.\ $\delta$
\STATE Set $m\leftarrow \max\bigl(m, \lceil C\,\varepsilon^{-2}(\log N+\log(1/\delta))\rceil\bigr)$
\STATE \textbf{Sample} $T\in\R^{m\times d}$ with $T_{ij}\sim\mathcal{N}(0,1/m)$
\STATE \textbf{Compute} $G\leftarrow TT^\Trans$ (shape $m\times m$); factor $G=LL^\Trans$ via Cholesky
\STATE \textbf{Left pseudo-inverse} $T^+ \leftarrow T^\Trans L^{-\Trans} L^{-1}$ \hfill (i.e., $T^\Trans (TT^\Trans)^{-1}$)
\STATE \textbf{Embeddings} $\tilde{E}\leftarrow E\,T^\Trans$
\STATE \textbf{Attention} For each layer: $\tilde{W}_Q\leftarrow T W_Q T^+$; same for $K,V,O$
\STATE \textbf{MLP} $\tilde{W}_\text{in}\leftarrow T W_\text{in}$;\quad $\tilde{W}_\text{out}\leftarrow W_\text{out}T^+$
\STATE \textbf{Norms} remove per-dim gains or replace by a scalar gain
\STATE \textbf{Positional} adjust RoPE dim to $\tilde{d}_\text{head}=m/n_\text{heads}$
\STATE \textbf{LM head} $\tilde{W}_\text{vocab}\leftarrow W_\text{vocab}T^+$ (or tie to $\tilde{E}$ if applicable)
\STATE \textbf{Fine-tune} with small LR and cosine decay to recover perplexity
\end{algorithmic}
\end{algorithm}

\subsection{Calibration set and guarantees in practice}
Choose $X$ to include: (i) all token embeddings (rows of $E$), (ii) steering
vectors or principal components of activations, and (iii) a large random sample
of hidden states from a small held-out corpus. The JL bound scales with
$\log N$, so tens to hundreds of thousands of vectors are feasible.

\section{Mechanistic metrics under the hypersphere lens}
\label{sec:metrics}
Let $\hat{u}=u/\norm{u}$ and $\widehat{Tu}=(Tu)/\norm{Tu}$. We propose:

\paragraph{Angular error (AE).}
For pairs $(u,v)$ in a probe set $\mathcal{P}$,
\[
\mathrm{AE} = \frac{1}{|\mathcal{P}|}\sum_{(u,v)\in\mathcal{P}}
\left|\,\arccos\ip{\hat{u}}{\hat{v}} - \arccos\ip{\widehat{Tu}}{\widehat{Tv}}\,\right|.
\]

\paragraph{Hyperspherical concordance (HC).}
Cosine correlation between pre/post similarity matrices on $\mathcal{P}$,
\[
\mathrm{HC} = \mathrm{corr}\bigl(\{\ip{\hat{u}_i}{\hat{u}_j}\}_{i<j},\;
\{\ip{\widehat{Tu}_i}{\widehat{Tv}_j}\}_{i<j}\bigr).
\]

\paragraph{Feature alignment (FA).}
Given a set of unit feature directions $\{w_k\}$ and their images
$\{Tw_k\}$ (unit-normalized), $\mathrm{FA}=\frac{1}{K}\sum_k \ip{\hat{w}_k}{\widehat{Tw}_k}$.

\paragraph{Kurtosis shift.}
For each weight/activation tensor, compare pre/post excess kurtosis; a drop
often improves uniform quantization.

\section{Experimental protocol (reproducible template)}
\label{sec:protocol}
\textbf{Models.} Small/medium decoder-only transformers (any open baseline).

\noindent\textbf{Datasets.} A standard LM corpus for fine-tuning and a held-out
validation set. A small calibration corpus for sampling hidden states.

\noindent\textbf{Procedure.} (1) Build $\mathcal{P}$ from embeddings + sampled
hidden states; (2) construct Gaussian $T$ (Alg.~\ref{alg:gaussian-jl}) for given
$m$; (3) port all width-touching tensors; (4) short fine-tune with AdamW or
Adafactor (LR $\sim\!1\!\times\!10^{-5}$ to $5\!\times\!10^{-5}$, cosine decay);
(5) evaluate PPL and geometry metrics (AE/HC/FA); (6) ablate norm gains and
head divisibility.

\noindent\textbf{Reporting.} Plot PPL vs.\ $m/d$; HC/AE vs.\ $m$; show representative
similarity heatmaps (pre/post). Report throughput (tokens/s) and memory.

\section{Applications}
\label{sec:applications}
\textbf{Shrinking (compression).} LGPT gives a teacher-free geometric
initialization, followed by short recovery. Useful when distillation is impractical.

\noindent\textbf{Same-size (quantization).} Global random projections reduce
kurtosis and coordinate concentration; evaluate PTQ/AWQ error before/after LGPT.

\noindent\textbf{Enlarging (staging).} Train at width $m$, then map to $d>m$
with a right-inverse $S$ (e.g., $S=T^\Trans$ for row-orthonormal $T$), and
continue training. This reuses compute for width growth.

\section{Ablations and limitations}
\label{sec:ablations}
\textbf{Norm gains.} Removing per-dim gains improves stability; otherwise,
$T\,\mathrm{Diag}(\gamma)\,T^+$ becomes dense.

\noindent\textbf{Heads.} Require $m$ divisible by $n_\text{heads}$, or adjust
$n_\text{heads}$; heterogeneous head sizes are possible but complicate code.

\noindent\textbf{RoPE.} Reset rotary dim to $\tilde{d}_\text{head}$; bases can
remain unchanged unless aggressively shrinking.

\noindent\textbf{Finite-set guarantee.} JL preserves geometry for the calibration
set; out-of-support activations may distort. Larger, more representative $\mathcal{P}$
improves empirical outcomes.

\noindent\textbf{Aggressive shrinkage.} Very small $m/d$ harms attention factorization
and logit expressivity; use staged shrinkage or modest $m/d$.

\section{Related work (brief)}
Random projections have a long history in dimensionality reduction and randomized
numerical linear algebra. JL-based maps (Gaussian, Rademacher, fast transforms)
provide strong finite-set guarantees. In interpretability, geometry-centric views
and circuit analyses motivate tools that maintain angular structure; LGPT offers
a principled, global basis change that can serve as a compression method and a
stress test for feature/circuit identity.

\section{Conclusion}
We introduced LGPT, a one-map JL resizing procedure that preserves latent geometry
for finite sets of interest under a hypersphere lens. It provides a commuting
construction for porting transformers to a new width, practical Gaussian-JL
pseudocode, and geometry-centric metrics. We propose LGPT both as a compression
primitive and as a mechanistic probe: features and circuits stable under global
basis changes are stronger candidates for genuine mechanistic structure.

\paragraph{Future work.} Combine LGPT with distillation to correct residual
distortions; explore structured/fast projections for very large $d$; design
benchmarks that require geometry preservation to succeed.

% ===========================
% References (minimal, generic)
% ===========================
\small
\bibliographystyle{unsrtnat}
\begin{thebibliography}{9}

\bibitem{johnson1984extensions}
W.~B. Johnson and J.~Lindenstrauss.
\newblock Extensions of Lipschitz mappings into a Hilbert space.
\newblock \emph{Contemporary Mathematics}, 26:189--206, 1984.

\bibitem{dasgupta2003elementary}
S.~Dasgupta and A.~Gupta.
\newblock An elementary proof of the Johnson--Lindenstrauss lemma.
\newblock \emph{International Computer Science Institute}, Tech. Report, 2003.

\bibitem{achlioptas2003database}
D.~Achlioptas.
\newblock Database-friendly random projections: Johnson--Lindenstrauss with binary coins.
\newblock \emph{Journal of Computer and System Sciences}, 66(4):671--687, 2003.

\bibitem{ailon2009fast}
N.~Ailon and B.~Chazelle.
\newblock The fast Johnson--Lindenstrauss transform and approximate nearest neighbors.
\newblock \emph{SIAM Journal on Computing}, 39(1):302--322, 2009.

\bibitem{sarlos2006improved}
T.~Sarl{\'o}s.
\newblock Improved approximation algorithms for large matrices via random projections.
\newblock \emph{FOCS}, 2006.

\end{thebibliography}

% ===========================
% Optional Appendix
% ===========================
\normalsize
\appendix
\section{Concept capacity heuristics on the hypersphere}
If we constrain coordinates to discrete alphabets (binary/ternary), the number of
distinct directions grows combinatorially. For intuition,
\begin{equation}
\sum_{k=1}^{\lfloor n/2 \rfloor} \binom{n}{k}
=
\begin{cases}
2^{\,n-1}-1, & \text{$n$ odd},\\[4pt]
2^{\,n-1}-1 + \dfrac{1}{2}\binom{n}{n/2}, & \text{$n$ even},
\end{cases}
\end{equation}
and by Stirling,
\begin{equation}
\binom{n}{n/2}\sim \frac{2^{\,n}}{\sqrt{\pi n/2}}.
\end{equation}
These counts motivate viewing width as a budget of (approximately) orthogonal
concept directions; quantization restricts the reachable subset of the sphere.

\end{document}
