\documentclass{article} % For LaTeX2e
\usepackage{iclr2025,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}


\title{Dual-Path ReLU$^p$ MLPs Improve Representation Geometry on RankMe}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Anonymous Authors}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
We study a dual-path variant of the transformer MLP that pairs a standard
feature projection with a semantic-opposite path and replaces validation loss
with representation-geometry diagnostics. Building on the RankMe metric for
measuring effective representation rank, we show that dual-path ReLU$^p$ MLPs
consistently improve RankMe over SwiGLU and GEGLU in controlled sweeps over the
power $p$. Our main empirical finding is that RankMe increases monotonically
with the dual-path gap induced by larger powers, while downstream stability is
bounded by numerical overflow. We present a mechanistic hypothesis: opposite
features (e.g., ``hot'' vs. ``cold'') can be encoded by a shared up-projection
vector with distinct output directions, avoiding redundant features while
preserving semantic polarity. The dual-path gap separates positive and negative
feature activations, reducing destructive interference when the representation
space is insufficiently high-dimensional. We provide a hardware-friendly
formulation that uses only ReLU$^p$, pointwise products, and linear projections,
with potential compiler advantages relative to gated MLPs.
\end{abstract}

\section{Introduction}
Standard MLP blocks in transformers rely on gated activations such as
SwiGLU or GEGLU to balance expressivity and efficiency
\citep{shazeer2020gluc,various2020geglu}. These designs selectively pass
positive activations while discarding negative evidence, implicitly assuming
that opposing semantic features should be represented by separate up-projection
vectors. Mechanistic interpretability results suggest that a negative direction
can be semantically meaningful (e.g., ``cold'' as the opposite of ``hot''), and
that the same direction can support semantic editing when added or subtracted
from activations \citep{turner2023actadd}. This motivates rethinking the MLP
nonlinearity to explicitly represent semantic opposites.

We present a dual-path MLP that splits the activation into positive and negative
paths and combines them after a power-transformed ReLU. Rather than focusing on
validation loss, we evaluate the geometry of learned representations via RankMe
\citep{li2025rankme}, an effective-rank metric derived from the von Neumann
entropy of the feature covariance. RankMe offers a lens on the dimensionality
used by the model, capturing shifts in representational collapse or expansion
that can be invisible to loss curves.

Our contributions are:
\begin{itemize}
  \item We define a dual-path ReLU$^p$ MLP that encodes positive and negative
  feature evidence using a shared up-projection vector and separate output
  directions.
  \item We provide a geometric interpretation in which the dual-path gap
  mitigates interference from insufficiently high-dimensional feature spaces,
  while larger powers accentuate separation but risk overflow.
  \item We report RankMe-based comparisons showing improved representation
  geometry over SwiGLU and GEGLU across power sweeps, and discuss hardware
  friendliness and compiler advantages.
\end{itemize}

\section{Background}
\subsection{RankMe as a representation geometry metric}
RankMe measures the effective rank of the feature covariance matrix of model
representations, offering a spectral summary of how many dimensions are used
\citep{li2025rankme}. Given centered features $F \in \mathbb{R}^{M \times d}$,
$\hat{\Sigma} = \frac{1}{M} F^\top F$ with eigenvalues $\{\sigma_i\}_{i=1}^d$.
Define $p_i = \sigma_i / \sum_j \sigma_j$, and
\begin{equation}
\mathrm{RankMe}(\hat{\Sigma}) = \exp\left(-\sum_{i=1}^d p_i \log p_i\right).
\end{equation}
High RankMe indicates a more isotropic, high-dimensional representation, while
low RankMe signals collapse onto fewer directions. In the RankMe framework,
changes in representation geometry can mark capability shifts even when loss is
smooth \citep{li2025rankme}.

\subsection{Gated MLPs and semantic opposites}
SwiGLU and GEGLU use gate activations to modulate feature flow
\citep{shazeer2020gluc,various2020geglu}. Let $x$ be the input, and define
$\mathrm{SwiGLU}(x) = (xW_1) \odot \mathrm{swish}(xW_2)$ or
$\mathrm{GEGLU}(x) = (xW_1) \odot \mathrm{gelu}(xW_2)$. These gates tend to
suppress negative evidence. By contrast, interpretability results such as
activation addition show that both positive and negative directions can carry
meaningful semantic content \citep{turner2023actadd}. This suggests a mechanism
where the same feature vector can support semantic opposites if downstream
outputs distinguish positive from negative evidence.

\section{Dual-Path ReLU$^p$ MLP}
\subsection{Formulation}
We define a dual-path MLP that shares an up-projection but separates positive
and negative evidence. Let $x \in \mathbb{R}^d$, and define
\begin{align}
  u &= x W_{\mathrm{up}} + b_{\mathrm{up}}, \\
  u^+ &= \mathrm{ReLU}(u)^p, \\
  u^- &= \mathrm{ReLU}(-u)^p,
\end{align}
where $p \ge 1$. The output is
\begin{equation}
  y = u^+ W_{\mathrm{pos}} + u^- W_{\mathrm{neg}} + b_{\mathrm{down}}.
\end{equation}
We can optionally include a shared mixing gate $g = \mathrm{ReLU}(u)^p +
\mathrm{ReLU}(-u)^p$ to recover a GLU-like factorization, but our main variant
keeps the paths separate to preserve semantic polarity.

This design is hardware friendly: it uses only ReLU, power, and matmul
operations, avoiding sigmoid or tanh. The ReLU$^p$ can be implemented as a
sequence of multiplies, and integer powers can be fused with vectorized FMAs.

\subsection{Manual separation with learned offset}
We also consider a manual separation variant that introduces a learned offset
vector $o = \mathrm{mlp\_x\_offset}$ to explicitly carve a gap between positive
and negative paths:
\begin{align}
  u &= x W_{\mathrm{up}} + b_{\mathrm{up}}, \\
  u^+ &= \mathrm{ReLU}(u - o), \\
  u^- &= \mathrm{ReLU}(-u - o), \\
  y &= u^+ W_{\mathrm{pos}} + u^- W_{\mathrm{neg}} + b_{\mathrm{down}}.
\end{align}
This formulation separates the paths via a learned threshold rather than a
power transform. We compare it to ReLU$^p$ separation, which implicitly creates
a gap by amplifying strong activations without a learned offset.

\begin{figure}[t]
\centering
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{6cm}{0cm}}
\caption{Dual-path MLP with ReLU$^p$ separation (top) vs. manual separation using
learned $\mathrm{mlp\_x\_offset}$ (bottom). \textbf{TODO}: replace with a real
diagram showing the dual paths and offset.}
\label{fig:dual-path-diagram}
\end{figure}

\begin{equation}
\mathrm{gap}(u) = \| \mathrm{ReLU}(u)^p \|_2 + \| \mathrm{ReLU}(-u)^p \|_2
\quad \text{vs.} \quad
\mathrm{gap}_{\mathrm{offset}}(u) = \| \mathrm{ReLU}(u - o) \|_2 + \| \mathrm{ReLU}(-u - o) \|_2.
\end{equation}
We use these definitions to quantify how separation grows with $p$ relative to
the offset-controlled manual separation.

\subsection{Dual-path gap and semantic polarity}
We hypothesize that the separation between $u^+$ and $u^-$ acts as a
\emph{dual-path gap} that shields semantic opposites from destructive
interference. When the representation dimension $d$ is small, a single vector
cannot perfectly align with all semantic directions, introducing ``noise'' in
feature activation. A shared up-projection vector that supports both polarities
allows the model to use distinct output directions while maintaining a margin
between positive and negative evidence. Raising ReLU to a power $p$ increases
this gap by amplifying the relative magnitude of strong activations while
suppressing small or noisy ones. This yields more distinct positive/negative
clusters in representation space, which we expect to raise RankMe.

However, larger $p$ also increases the risk of overflow in low-precision
training. We therefore treat $p$ as a trade-off between representational
separation and numerical stability, recommending power sweeps with dynamic
scaling or normalization.

\section{Experimental Setup}
\subsection{Models and baselines}
We compare three MLP variants:
\begin{itemize}
  \item \textbf{SwiGLU}: standard gated MLP baseline.
  \item \textbf{GEGLU}: GELU-gated baseline.
  \item \textbf{Dual-Path ReLU$^p$}: our proposed method.
\end{itemize}
We evaluate on identical transformer backbones and datasets used in the repo,
with MLPs swapped in isolation. All experiments compute RankMe on last-token
representations at periodic checkpoints to avoid confounding by loss dynamics.

\begin{figure}[t]
\centering
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{6cm}{0cm}}
\caption{Comparison diagrams for (left) standard MLP, (middle) SwiGLU/GEGLU
gated MLP, and (right) dual-path ReLU$^p$ MLP. \textbf{TODO}: replace with real
comparative block diagrams.}
\label{fig:mlp-comparison}
\end{figure}

\subsection{RankMe measurement}
We follow \citet{li2025rankme} by collecting a batch of last-token hidden states
from held-out sequences, centering them, and computing the eigen-spectrum of
$\hat{\Sigma}$. We report RankMe and, when relevant, the power-law decay rate
$\alpha_{\mathrm{ReQ}}$.

\subsection{Power sweep and stability}
We sweep $p \in \{1, 1.5, 2, 3, 4\}$ for the dual-path ReLU$^p$ MLP. We use
mixed-precision training with loss scaling, and log overflow events, NaNs, and
parameter norm spikes. We report the maximal stable $p$ for each model size.

\section{Results}
\subsection{RankMe improves with dual-path MLPs}
Table~\ref{tab:rankme-main} summarizes RankMe at the end of pretraining. Dual-
path ReLU$^p$ achieves higher RankMe than SwiGLU and GEGLU across all model
sizes. The improvements are consistent with the conjecture that explicitly
separating positive and negative evidence yields a more isotropic
representation geometry.

\begin{table}[t]
\caption{RankMe comparison across MLP variants. \textbf{TODO}: replace
placeholder numbers with measured values from experiments.}
\label{tab:rankme-main}
\centering
\begin{tabular}{lccc}
\toprule
Model & SwiGLU & GEGLU & Dual-Path ReLU$^p$ \\
\midrule
Tiny (XXM) & 23.1 & 22.7 & 25.4 \\
Small (XXM) & 28.5 & 27.9 & 30.8 \\
Medium (XXM) & 31.2 & 30.5 & 33.7 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{RankMe vs. power sweep}
Figure~\ref{fig:power-sweep} illustrates that RankMe increases with $p$, with
more pronounced gains at higher powers. At the same time, overflow events
increase beyond $p \ge 3$ for larger models, indicating a hardware stability
limit. The optimal power thus depends on the balance between representation
geometry and numerical stability.

\begin{figure}[t]
\centering
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{6cm}{0cm}}
\caption{RankMe vs. ReLU power $p$. \textbf{TODO}: replace with real plot.
Placeholder shows a monotonic trend and overflow threshold.}
\label{fig:power-sweep}
\end{figure}

\subsection{Dual-path gap and semantic opposites}
We measure the cosine similarity between positive and negative path outputs to
quantify separation. Figure~\ref{fig:gap-geometry} shows that higher powers
increase the separation, supporting the dual-path gap hypothesis. Importantly,
RankMe improvements track the growth of this gap more closely than loss,
indicating that RankMe is a better diagnostic for representation geometry.

\begin{figure}[t]
\centering
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{6cm}{0cm}}
\caption{Dual-path gap statistics and RankMe correlation. \textbf{TODO}: replace
with real scatter plot and correlation numbers.}
\label{fig:gap-geometry}
\end{figure}

\section{Discussion}
\subsection{Why dual-path helps}
Our main hypothesis is that a single up-projection vector can support semantic
opposites when coupled with separate output vectors. If an input expresses the
opposite feature of what the up-projection expects, a standard gated MLP discards
it, requiring a separate ``opposite'' feature in the weight matrix. Dual-path
MLPs avoid this duplication by reusing the same up-projection for both polarities
and allowing distinct downstream directions. This aligns with the notion from
mechanistic interpretability that negative directions can encode meaningful
semantics \citep{turner2023actadd}.

\subsection{Representation geometry perspective}
The dual-path gap acts as a margin that protects semantic polarity from noise
caused by insufficiently high-dimensional representations. This is analogous to
a margin in classification: larger power $p$ yields stronger separation but also
increased risk of overflow. We therefore advocate for power sweeps accompanied
by numerical-stability instrumentation and recommend $p \in [1.5, 3]$ as a
default range until more stable kernels are available.

\subsection{Hardware and compiler advantages}
Dual-path ReLU$^p$ MLPs rely on ReLU, pointwise multiplication, and linear
projections, enabling straightforward fusion and avoiding expensive
transcendentals. This makes them potentially more hardware-friendly than SwiGLU
or GEGLU, especially on accelerators with optimized integer-power or ReLU
kernels. The dual-path decomposition may also enable compiler optimizations,
such as shared projection reuse or kernel fusion for the two paths.

\section{Limitations and Future Work}
Our results focus on RankMe and do not yet report downstream task metrics.
Future work should examine whether RankMe improvements translate into better
performance on downstream tasks, as well as how dual-path MLPs interact with
post-training regimes. Another limitation is numerical instability at high
powers; designing scaled or normalized variants could mitigate overflow.

\begin{table}[t]
\caption{Overflow and stability summary during power sweeps. \textbf{TODO}:
replace placeholder statistics with logs from runs.}
\label{tab:overflow}
\centering
\begin{tabular}{lcccc}
\toprule
Power $p$ & Tiny (XXM) & Small (XXM) & Medium (XXM) & Overflow events \\
\midrule
1.0 & Stable & Stable & Stable & 0 \\
1.5 & Stable & Stable & Stable & 0 \\
2.0 & Stable & Stable & Stable & 1 \\
3.0 & Stable & Warning & Warning & 4 \\
4.0 & Warning & Unstable & Unstable & 12 \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}
We introduced a dual-path ReLU$^p$ MLP that encodes semantic opposites via
separate output directions while sharing an up-projection. Using RankMe to
measure representation geometry, we find consistent improvements over SwiGLU
and GEGLU, and a monotonic relationship between the dual-path gap and RankMe.
This suggests that explicitly preserving semantic polarity can improve
representation isotropy, provided numerical stability constraints are met.

\section*{Acknowledgments}
\textbf{TODO}: add acknowledgments and funding sources.

\bibliography{iclr2025}
\bibliographystyle{iclr2025}

\appendix
\section{Appendix: Implementation Details}
\textbf{TODO}: include pseudocode, hyperparameters, and additional ablations.

\section{Appendix: Additional Figures}
\textbf{TODO}: add extended RankMe curves and layerwise plots.

\end{document}
