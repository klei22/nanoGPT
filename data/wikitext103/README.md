# Wikitext103

The wikitext103 dataset is acollection +100 million tokens from a set of "Good and Featured" articles from Wikipedia.
This has been widely used for language modeling with credits to:

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. [Pointer Sentinel Mixture Models](https://arxiv.org/abs/1609.07843)

Please follow the following link for more information:
- [Direct Link to Dataset Website](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/#download)
