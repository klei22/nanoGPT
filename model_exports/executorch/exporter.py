"""Utilities to export nanoGPT checkpoints to ExecuTorch ``.pte`` programs."""

from __future__ import annotations

import copy
import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import torch

from gpt_conf import GPTConfig
from model import GPT


@dataclass(slots=True)
class ExportConfig:
    """Configuration parameters that control ExecuTorch exports."""

    delegate: str = "none"
    generate_etrecord: bool = False
    smoke_test_tokens: int = 0
    smoke_test_prompt: Optional[str] = None
    tokenizer_path: Optional[Path] = None
    max_output_tokens: int = 32
    metadata: bool = True

    def validate(self) -> None:
        if self.delegate not in {"none", "xnnpack"}:
            raise ValueError(f"Unsupported delegate '{self.delegate}'.")
        if self.smoke_test_tokens < 0:
            raise ValueError("smoke_test_tokens must be non-negative.")
        if self.max_output_tokens <= 0:
            raise ValueError("max_output_tokens must be positive.")


def _infer_vocab_size(model_args: dict) -> int:
    vocab_size = model_args.get("vocab_size")
    if vocab_size is None:
        raise ValueError("Model arguments do not include 'vocab_size'.")
    return int(vocab_size)


def _infer_block_size(model_args: dict) -> int:
    block_size = model_args.get("block_size")
    if block_size is None:
        raise ValueError("Model arguments do not include 'block_size'.")
    return int(block_size)


def _prepare_state_dict(state_dict: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:
    """Normalize checkpoint parameter keys so they load with the current model."""

    prepared = dict(state_dict)
    prepared = _strip_module_prefix(prepared)
    prepared = _convert_legacy_attention_weights(prepared)
    return prepared


def _strip_module_prefix(state_dict: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:
    """Drop DistributedDataParallel's ``module.`` prefix when present."""

    if not state_dict:
        return state_dict

    if all(key.startswith("module.") for key in state_dict):
        return {key[len("module."):]: value for key, value in state_dict.items()}

    return state_dict


def _convert_legacy_attention_weights(
    state_dict: dict[str, torch.Tensor]
) -> dict[str, torch.Tensor]:
    """Split combined QKV projections saved by older checkpoints."""

    legacy_keys = [key for key in state_dict if key.endswith("attn.c_attn.weight")]
    if not legacy_keys:
        return state_dict

    updated = dict(state_dict)
    for weight_key in legacy_keys:
        bias_key = weight_key.replace(".weight", ".bias")

        weight = updated.pop(weight_key)
        bias = updated.pop(bias_key, None)

        try:
            q_weight, k_weight, v_weight = weight.chunk(3, dim=0)
        except RuntimeError as err:  # pragma: no cover - defensive path
            raise RuntimeError(
                f"Failed to split legacy attention weights for '{weight_key}': {err}"
            ) from err

        base = weight_key.replace("c_attn.weight", "c_attn_")
        updated[f"{base}q.weight"] = q_weight
        updated[f"{base}k.weight"] = k_weight
        updated[f"{base}v.weight"] = v_weight

        if bias is not None:
            q_bias, k_bias, v_bias = bias.chunk(3, dim=0)
            bias_base = bias_key.replace("c_attn.bias", "c_attn_")
            updated[f"{bias_base}q.bias"] = q_bias
            updated[f"{bias_base}k.bias"] = k_bias
            updated[f"{bias_base}v.bias"] = v_bias

    return updated


def export_checkpoint_to_pte(
    ckpt_path: os.PathLike | str,
    output_path: os.PathLike | str,
    export_config: Optional[ExportConfig] = None,
) -> Path:
    """Convert ``ckpt.pt`` files generated by nanoGPT into ExecuTorch ``.pte`` files."""

    export_config = export_config or ExportConfig()
    export_config.validate()

    ckpt_path = Path(ckpt_path)
    output_path = Path(output_path)
    if ckpt_path.suffix == "":
        raise ValueError("Checkpoint path must include a filename, not just a directory.")
    if not ckpt_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {ckpt_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    checkpoint = torch.load(ckpt_path, map_location="cpu")
    if "model" not in checkpoint:
        raise KeyError("Checkpoint does not contain 'model' weights.")
    if "model_args" not in checkpoint:
        raise KeyError("Checkpoint does not contain 'model_args'.")

    model_args = checkpoint["model_args"]
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)

    state_dict = _prepare_state_dict(checkpoint["model"])
    missing, unexpected = model.load_state_dict(state_dict, strict=False)
    if missing or unexpected:
        raise RuntimeError(
            "Checkpoint parameters do not match the GPT architecture. "
            f"Missing keys: {sorted(missing)}; Unexpected keys: {sorted(unexpected)}"
        )
    model.eval()

    vocab_size = _infer_vocab_size(model_args)
    block_size = _infer_block_size(model_args)

    example_inputs = (
        torch.randint(0, vocab_size, (1, block_size), dtype=torch.long),
    )
    dynamic_shape = (
        {1: torch.export.Dim("token_dim", max=block_size)},
    )

    from torch.nn.attention import SDPBackend, sdpa_kernel
    from torch.export import export, export_for_training

    with sdpa_kernel([SDPBackend.MATH]), torch.no_grad():
        training_program = export_for_training(
            model, example_inputs, dynamic_shapes=dynamic_shape
        ).module()
        traced_program = export(training_program, example_inputs, dynamic_shapes=dynamic_shape)

    if export_config.delegate == "xnnpack":
        from executorch.backends.xnnpack.partition.xnnpack_partitioner import (
            XnnpackPartitioner,
        )
        from executorch.backends.xnnpack.utils.configs import get_xnnpack_edge_compile_config
        from executorch.exir import to_edge_transform_and_lower

        edge_config = get_xnnpack_edge_compile_config()
        edge_manager = to_edge_transform_and_lower(
            traced_program,
            partitioner=[XnnpackPartitioner()],
            compile_config=edge_config,
        )
    else:
        from executorch.exir import EdgeCompileConfig, to_edge

        edge_config = EdgeCompileConfig(_check_ir_validity=False)
        edge_manager = to_edge(traced_program, compile_config=edge_config)

    edge_manager_copy = copy.deepcopy(edge_manager) if export_config.generate_etrecord else None
    et_program = edge_manager.to_executorch()

    output_path.write_bytes(et_program.buffer)

    if export_config.generate_etrecord and edge_manager_copy is not None:
        from executorch.devtools import generate_etrecord

        etrecord_path = output_path.with_suffix(output_path.suffix + ".etrecord")
        generate_etrecord(str(etrecord_path), edge_manager_copy, et_program)

    if export_config.metadata:
        metadata_path = output_path.with_suffix(output_path.suffix + ".json")
        metadata = {
            "checkpoint": str(ckpt_path.resolve()),
            "pte": str(output_path.resolve()),
            "delegate": export_config.delegate,
            "generate_etrecord": export_config.generate_etrecord,
            "vocab_size": vocab_size,
            "block_size": block_size,
            "max_output_tokens": export_config.max_output_tokens,
        }
        metadata_path.write_text(json.dumps(metadata, indent=2))

    if export_config.smoke_test_tokens:
        _smoke_test_export(
            output_path,
            block_size=block_size,
            vocab_size=vocab_size,
            num_tokens=export_config.smoke_test_tokens,
        )

    if export_config.smoke_test_prompt:
        _smoke_test_prompt(
            output_path,
            prompt=export_config.smoke_test_prompt,
            max_input_length=block_size,
            tokenizer_path=export_config.tokenizer_path,
            max_output_tokens=export_config.max_output_tokens,
        )

    return output_path


def _smoke_test_export(
    pte_path: Path, *, block_size: int, vocab_size: int, num_tokens: int
) -> None:
    import numpy as np

    from executorch.extension.module import Module
    from executorch.extension.tensor import from_numpy

    module = Module(str(pte_path))
    max_tokens = min(block_size, max(1, num_tokens))
    tokens = np.random.randint(
        0,
        max(vocab_size, 1),
        size=(1, max_tokens),
        dtype=np.int64,
    )
    inputs = from_numpy(tokens)
    module.forward(inputs)


def _smoke_test_prompt(
    pte_path: Path,
    prompt: str,
    max_input_length: int,
    tokenizer_path: Optional[Path],
    max_output_tokens: int,
) -> None:
    if tokenizer_path is None:
        raise ValueError("A tokenizer vocabulary is required to run prompt-based smoke tests.")

    import json as json_lib

    import numpy as np

    from executorch.extension.module import Module
    from executorch.extension.tensor import from_numpy

    vocab_map = json_lib.loads(Path(tokenizer_path).read_text())
    token_ids: list[int] = []
    if isinstance(vocab_map, dict):
        if prompt in vocab_map:
            token_ids.append(int(vocab_map[prompt]))
        else:
            for piece in prompt.split():
                token_ids.append(int(vocab_map.get(piece, 0)))
    if not token_ids:
        token_ids = [0]

    token_array = np.array([token_ids[:max_input_length]], dtype=np.int64)
    _ = max_output_tokens  # placeholder to document intent for decode length handling

    module = Module(str(pte_path))
    inputs = from_numpy(token_array)
    outputs = module.forward(inputs)

    if not outputs:
        raise RuntimeError("ExecuTorch module returned no outputs during smoke test.")
