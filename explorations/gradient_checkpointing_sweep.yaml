# explorations/gradient_checkpointing_sweep.yaml
---
# Study VRAM and iteration speed trade-offs when using gradient checkpointing.
# Activation memory per layer ≈ batch_size * block_size * n_embd * bytes_per_elem.
# Without checkpointing: activations scale with n_layer -> VRAM ≈ L * S.
# With checkpointing: only boundary activations stored -> ≈0.5 × VRAM but requires recomputation (~25% slower).
# PyTorch compile fuses ops, giving ~30% faster iterations but ~10% more VRAM; combining with checkpointing recovers speed while retaining memory savings.

parameter_groups:
  - compile: [false]
    use_gradient_checkpointing: [false]  # baseline
  - compile: [false]
    use_gradient_checkpointing: [true]   # ≈50% less activation VRAM, ≈25% slower
  - compile: [true]
    use_gradient_checkpointing: [false]  # ≈30% faster, ≈10% more VRAM
  - compile: [true]
    use_gradient_checkpointing: [true]   # ≈55% less VRAM, speed ≈ baseline

# base hyperparameters
max_iters: [500]
batch_size: [2, 4, 6]
n_head: [6]
dataset: ["shakespeare_char"]
device: ["cuda"]
dtype: ["bfloat16"]

# Position Encoding
use_rotary_embeddings: [true]
use_abs_pos_embeddings: [false]

# sweep variables
n_layer: [2, 4, 6]           # memory scales linearly with layers
n_embd: [120, 240, 360]      # wider models increase activation memory; checkpointing saves proportionally
block_size: [64, 128, 192]   # attention memory scales ~block_size^2; checkpointing can't reduce quadratic term

# checkpoint settings
never_save_checkpoint: [true]
