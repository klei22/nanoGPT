# training_limits.yaml demonstrating token/epoch limits, eval intervals, and final evals
---

common_group:
  dataset: ["shakespeare_char"]
  n_layer: ["2"]
  n_head: ["2"]
  n_embd: ["128"]
  block_size: ["64"]
  batch_size: ["8"]
  dtype: ["bfloat16"]
  device: ["cuda"]
  never_save_checkpoint: [true]

parameter_groups:
  - named_group: "token_limited_eval"
    max_tokens: [50000]
    eval_interval_tokens: [10000.0]
    final_eval: [true]
    final_eval_save_checkpoint: [true]
    never_save_checkpoint: [false]
  - named_group: "epoch_limited_eval"
    max_epochs: [1.5]
    eval_interval_epochs: [0.25]
    final_eval: [true]
  - named_group: "multidataset_main_tokens"
    training_mode: ["multidataset"]
    dataset_list: ["shakespeare_char wikitext103"]
    max_tokens: [30000]
    eval_interval_tokens: [7500.0]
