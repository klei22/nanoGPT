# explorations/loop_penalty_comparison.yaml
---
# Compare the loop-penalty loss with other recent top-1 focused objectives.

common_group:
  # Training run basics
  dataset: ["minipile"]
  max_iters: [8000]
  eval_interval: [500]
  log_interval: [50]
  block_size: [256]
  batch_size: [48]
  n_layer: [6]
  n_head: [6]
  n_embd: [384]
  lr_decay_iters: [8000]
  device: ["cuda"]
  dtype: ["bfloat16"]
  compile: [true]
  sample_each_eval: [true]
  max_sample_tokens: ["256"]
  # Recommended rotary/qk-norm defaults for recent experiments
  use_rotary_embeddings: [true]
  use_abs_pos_embeddings: [false]
  use_qk_norm: [true]
  use_qk_norm_scale: [true]
  # Avoid cluttering checkpoint directories when sweeping
  never_save_checkpoint: [true]

# Loss variants under test. Each block defines one set of overrides to try.
parameter_groups:
  - loss_fn: ["cross_entropy"]

  - loss_fn: ["loop_penalty"]
    loop_penalty_strength: [0.1, 0.2]
    loop_penalty_window: [3, 5]
    # Provide a sensible default newline boost; set loop_penalty_newline_id to
    # the tokenizer-specific value (198 for GPT-2 BPE) when launching runs.
    loop_penalty_newline_multiplier: [1.5]

  - loss_fn: ["skip_correct_top1"]

  - loss_fn: ["attenuated_correct_top1"]
    correct_top1_attenuation: [0.25, 0.5]

  - loss_fn: ["distance_attenuated_top1"]
    distance_top1_strength: [0.25, 0.5]

  - loss_fn: ["top1_margin"]
    top1_margin: [0.05, 0.1]

  - loss_fn: ["focal"]
    focal_gamma: [1.5, 2.5]
