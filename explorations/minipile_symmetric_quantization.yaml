# minipile_symmetric_quantization.yaml
---
# Compare baseline training vs symmetric 4-bit weight quantization from the start on minipile

parameter_groups:
  # Baseline: standard linear layers without quantization
  - tensorboard_run_name: ["minipile_fp16_baseline"]
    linear_variant_attn: ["linear"]
    linear_variant_mlp: ["linear"]

  # Symmetric 4-bit weight quantization applied from the first iteration
  - tensorboard_run_name: ["minipile_symm_w4"]
    linear_variant_attn: ["quantized_linear"]
    linear_variant_mlp: ["quantized_linear"]
    quantize_linear_method: ["symmetric_quant"]
    quantize_linear_bits: [4]
    full_quant_iteration: [10000]
    start_quant_level: [1.0]
    quant_scheduler: ["static"]
    quantization_warmup_iters: [0]

# Shared training configuration
max_iters: [10000]
lr_decay_iters: [10000]
eval_interval: [1000]
log_interval: [10]
batch_size: [64]
block_size: [256]
n_layer: [6]
n_head: [6]
n_embd: [384]
dataset: ["minipile"]
device: ["cuda"]
dtype: ["bfloat16"]
compile: [true]
learning_rate: ["3e-4"]
beta1: [0.9]
beta2: [0.95]
adamw_weight_decay: [0.1]
adamw_eps: ["1e-8"]
use_rotary_embeddings: [true]
use_abs_pos_embeddings: [false]
softmax_variant_attn: ["softmax"]
softmax_variant_output: ["softmax"]
