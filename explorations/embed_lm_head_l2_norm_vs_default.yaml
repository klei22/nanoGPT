# explorations/embed_lm_head_l2_norm_vs_default.yaml
---
parameter_groups:
  # baseline without L2 normalization
  - l2_norm_embed: [false]
    l2_norm_lm_head: [false]

  # only L2 normalizing token embeddings
  - l2_norm_embed: [true]
    l2_norm_embed_scale: [null, 1.0, 10.0]
    l2_norm_embed_scale_learnable: [true, false]
    l2_norm_lm_head: [false]

  # only L2 normalizing LM head weights
  - l2_norm_embed: [false]
    l2_norm_lm_head: [true]
    l2_norm_lm_head_scale: [null, 1.0, 10.0]
    l2_norm_lm_head_scale_learnable: [true, false]

  # L2 normalizing both embeddings and LM head weights
  - l2_norm_embed: [true]
    l2_norm_embed_scale: [null, 1.0, 10.0]
    l2_norm_embed_scale_learnable: [true, false]
    l2_norm_lm_head: [true]
    l2_norm_lm_head_scale: [null, 1.0, 10.0]
    l2_norm_lm_head_scale_learnable: [true, false]

# base hyperparameters
max_iters: [10000]
eval_interval: [10000]
eta_variant: ["iteration"]
n_layer: [12]
n_head: [12]
n_embd: [768]
batch_size: [16]
block_size: [1024]
device: ["cuda"]
dtype: ["float16"]
dataset: ["minipile"]

# training options
use_rotary_embeddings: [true]
use_abs_pos_embeddings: [false]
use_qk_norm: [true]
use_qk_norm_scale: [true]
use_peri_ln: [true]
compile: [true]
compute_model_stats: [true]
print_model_stats_table: ["./print_stats/${RUN_NAME}.csv"]

# VRAM and Memory Saving
never_save_checkpoint: [true]
